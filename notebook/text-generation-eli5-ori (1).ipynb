{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Library\n","metadata":{}},{"cell_type":"code","source":"import functools\nimport math\nimport os  # noqa: F401\nfrom random import choice, randint\nfrom time import time\n\nimport numpy as np\nimport torch\nimport torch.utils.checkpoint as checkpoint\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom tqdm import tqdm\n\nimport json\nfrom transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer,  get_linear_schedule_with_warmup\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:10:47.589617Z","iopub.execute_input":"2023-02-12T22:10:47.590080Z","iopub.status.idle":"2023-02-12T22:10:48.158185Z","shell.execute_reply.started":"2023-02-12T22:10:47.590035Z","shell.execute_reply":"2023-02-12T22:10:48.157263Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"with open(r'/kaggle/input/eli5-10-doc/ELI5_train_10_doc.json') as f:\n    train = json.load(f)\n    \nwith open(r'/kaggle/input/eli5-10-doc/ELI5_val_10_doc.json') as f:\n    val = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:27:29.448808Z","iopub.execute_input":"2023-02-12T00:27:29.449473Z","iopub.status.idle":"2023-02-12T00:28:04.931501Z","shell.execute_reply.started":"2023-02-12T00:27:29.449441Z","shell.execute_reply":"2023-02-12T00:28:04.930481Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:40:22.269037Z","iopub.execute_input":"2023-02-11T05:40:22.269424Z","iopub.status.idle":"2023-02-11T05:40:22.294431Z","shell.execute_reply.started":"2023-02-11T05:40:22.269384Z","shell.execute_reply":"2023-02-11T05:40:22.293520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pretrained DPR","metadata":{}},{"cell_type":"code","source":"# qar_tokenizer = AutoTokenizer.from_pretrained('yjernite/retribert-base-uncased')\n# qar_model = AutoModel.from_pretrained('yjernite/retribert-base-uncased').to('cuda:1')\n# _ = qar_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-02-02T09:20:35.955683Z","iopub.execute_input":"2023-02-02T09:20:35.956436Z","iopub.status.idle":"2023-02-02T09:20:58.431869Z","shell.execute_reply.started":"2023-02-02T09:20:35.956380Z","shell.execute_reply":"2023-02-02T09:20:58.430782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing data\n1. Embedding passsage\n2. Embedding question","metadata":{}},{"cell_type":"code","source":"# def embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length=128, device=\"cuda:1\"):\n#     a_toks = tokenizer.batch_encode_plus(passages, max_length=max_length, pad_to_max_length=True)\n#     a_ids, a_mask = (\n#         torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n#         torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n#     )\n#     with torch.no_grad():\n#         a_reps = qa_embedder.embed_answers(a_ids, a_mask).cpu().type(torch.float)\n#     return a_reps.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-02-02T04:42:25.339554Z","iopub.execute_input":"2023-02-02T04:42:25.341074Z","iopub.status.idle":"2023-02-02T04:42:25.348756Z","shell.execute_reply.started":"2023-02-02T04:42:25.341034Z","shell.execute_reply":"2023-02-02T04:42:25.347719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def embed_questions_for_retrieval(q_ls, tokenizer, qa_embedder, device=\"cuda:1\"):\n#     q_toks = tokenizer.batch_encode_plus(q_ls, max_length=128, pad_to_max_length=True)\n#     q_ids, q_mask = (\n#         torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n#         torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n#     )\n#     with torch.no_grad():\n#         q_reps = qa_embedder.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n#     return q_reps.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-02-02T04:42:25.351468Z","iopub.execute_input":"2023-02-02T04:42:25.351908Z","iopub.status.idle":"2023-02-02T04:42:25.364110Z","shell.execute_reply.started":"2023-02-02T04:42:25.351870Z","shell.execute_reply":"2023-02-02T04:42:25.363019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = val[0]\n# support_doc = \"<P> \" + \" <P> \".join([str(p) for p in sample['ctxs']])\n# support_doc","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:29:27.709171Z","iopub.execute_input":"2023-02-02T06:29:27.709560Z","iopub.status.idle":"2023-02-02T06:29:27.716699Z","shell.execute_reply.started":"2023-02-02T06:29:27.709528Z","shell.execute_reply":"2023-02-02T06:29:27.715633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def format_dataset(dataset: list):\n#     for sample in dataset:\n#         sample['ctxs'] = \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in sample['ctxs']])\n#     return dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-02T04:42:25.365722Z","iopub.execute_input":"2023-02-02T04:42:25.366166Z","iopub.status.idle":"2023-02-02T04:42:25.373839Z","shell.execute_reply.started":"2023-02-02T04:42:25.366128Z","shell.execute_reply":"2023-02-02T04:42:25.372730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val[0]","metadata":{"execution":{"iopub.status.busy":"2023-02-02T04:36:27.142383Z","iopub.execute_input":"2023-02-02T04:36:27.142835Z","iopub.status.idle":"2023-02-02T04:36:27.152376Z","shell.execute_reply.started":"2023-02-02T04:36:27.142796Z","shell.execute_reply":"2023-02-02T04:36:27.151300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_format = format_dataset(train[0:172634])\n# val_format = format_dataset(val[0:5])\n# val_format","metadata":{"execution":{"iopub.status.busy":"2023-02-02T07:39:02.243599Z","iopub.execute_input":"2023-02-02T07:39:02.244714Z","iopub.status.idle":"2023-02-02T07:39:02.252597Z","shell.execute_reply.started":"2023-02-02T07:39:02.244648Z","shell.execute_reply":"2023-02-02T07:39:02.251619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for sample in val:\n#     sample['ctxs'] = embed_passages_for_retrieval(sample['ctxs'], qar_tokenizer, qar_model)\n# val[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-02-02T04:34:32.702446Z","iopub.execute_input":"2023-02-02T04:34:32.702821Z","iopub.status.idle":"2023-02-02T04:34:32.707157Z","shell.execute_reply.started":"2023-02-02T04:34:32.702789Z","shell.execute_reply":"2023-02-02T04:34:32.706138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class ArgumentsS2S():\n    def __init__(self):\n        self.batch_size = 8\n        self.backward_freq = 16\n        self.max_length = 1024\n        self.print_freq = 100\n        self.model_save_name = \"seq2seq_models/eli5_bart_model\"\n        self.learning_rate = 2e-4\n        self.num_epochs = 1","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:28:04.932903Z","iopub.execute_input":"2023-02-12T00:28:04.933292Z","iopub.status.idle":"2023-02-12T00:28:04.940692Z","shell.execute_reply.started":"2023-02-12T00:28:04.933255Z","shell.execute_reply":"2023-02-12T00:28:04.939337Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def make_qa_s2s_model(model_name=\"facebook/bart-large\", from_file=None, device=\"cuda:0\"):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    if from_file is not None:\n        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n        model.load_state_dict(param_dict[\"model\"])\n    return tokenizer, model","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:10:57.502426Z","iopub.execute_input":"2023-02-12T22:10:57.502801Z","iopub.status.idle":"2023-02-12T22:10:57.509234Z","shell.execute_reply.started":"2023-02-12T22:10:57.502753Z","shell.execute_reply":"2023-02-12T22:10:57.508052Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=360, device=\"cuda:0\"):\n    q_ls = [q for q, a in qa_list]\n    a_ls = [a for q, a in qa_list]\n    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n    q_ids, q_mask = (\n        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n    )\n    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True)\n    a_ids, a_mask = (\n        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n    )\n    labels = a_ids[:, 1:].contiguous().clone()\n    labels[a_mask[:, 1:].contiguous() == 0] = -100\n    model_inputs = {\n        \"input_ids\": q_ids,\n        \"attention_mask\": q_mask,\n        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n        \"labels\": labels,\n    }\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:10:57.908219Z","iopub.execute_input":"2023-02-12T22:10:57.908934Z","iopub.status.idle":"2023-02-12T22:10:57.919232Z","shell.execute_reply.started":"2023-02-12T22:10:57.908890Z","shell.execute_reply":"2023-02-12T22:10:57.918250Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=False):\n    model.train()\n    # make iterator\n    if curriculum:\n        train_sampler = SequentialSampler(dataset)\n    else:\n        train_sampler = RandomSampler(dataset)\n    # create more args (extra args)\n    model_collate_fn = functools.partial(\n        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n        ####\n    )\n    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    for step, batch_inputs in enumerate(epoch_iterator):\n        ############ NOTE\n        pre_loss = model(**batch_inputs)[0]\n#         print(pre_loss)\n        loss = pre_loss.sum() / pre_loss.shape[0]\n        loss.backward()\n        # optimizer\n        if step % args.backward_freq == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        # some printing within the epoch\n        loc_loss += loss.item()\n        loc_steps += 1\n        if step % args.print_freq == 0 or step == 1:\n            print(\n                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n                )\n            )\n            loc_loss = 0\n            loc_steps = 0\n","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:28:04.970391Z","iopub.execute_input":"2023-02-12T00:28:04.971221Z","iopub.status.idle":"2023-02-12T00:28:04.982854Z","shell.execute_reply.started":"2023-02-12T00:28:04.971179Z","shell.execute_reply":"2023-02-12T00:28:04.981479Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n    s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n    s2s_scheduler = get_linear_schedule_with_warmup(\n        s2s_optimizer,\n        num_warmup_steps=400,\n        num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n    )\n    for e in range(s2s_args.num_epochs):\n        train_qa_s2s_epoch(\n            qa_s2s_model,\n            s2s_train_dset,\n            qa_s2s_tokenizer,\n            s2s_optimizer,\n            s2s_scheduler,\n            s2s_args,\n            e,\n            curriculum=(e == 0),\n        )\n        m_save_dict = {\n            \"model\": qa_s2s_model.module.state_dict()\n                    if hasattr(qa_s2s_model, \"module\") else qa_s2s_model.state_dict(),\n            \"optimizer\": s2s_optimizer.state_dict(),\n            \"scheduler\": s2s_scheduler.state_dict(),\n        }\n        print(\"Saving model {}\".format(s2s_args.model_save_name))\n        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n        torch.save(m_save_dict, \"bart-base_model.pth\")\n    writer.flush()\n    print('Done!')","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:28:04.984737Z","iopub.execute_input":"2023-02-12T00:28:04.985232Z","iopub.status.idle":"2023-02-12T00:28:04.998203Z","shell.execute_reply.started":"2023-02-12T00:28:04.985194Z","shell.execute_reply":"2023-02-12T00:28:04.997147Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ELI5DatasetS2S(Dataset):\n    def __init__(\n        self, examples_array, make_doc_fun=None, extra_answer_threshold=3, document_cache=None, training=True\n    ):\n        self.training = training\n        self.data = examples_array\n        self.make_doc_function = make_doc_fun\n            \n        self.document_cache = {} if document_cache is None else document_cache\n        \n        assert not (make_doc_fun is None and document_cache is None)\n        # make index of specific question-answer pairs from multi-answers\n        # Fix: just using only one answer for training \n        # Not require self.training\n        if self.training:\n            self.qa_id_list = [\n                (i, j)\n                for i, qa in enumerate(self.data)\n                for j, a in enumerate(qa[\"answers\"])\n#                 if j == 0 or sc >= extra_answer_threshold\n            ]\n        else:\n            self.qa_id_list = [(i, 0) for i in range(len(self.data))]\n#         self.qa_id_list = [(i, 0) for i in range(len(self.data))]\n\n    def __len__(self):\n        return len(self.qa_id_list)\n\n    def make_example(self, idx):\n        i, j = self.qa_id_list[idx]\n        example = self.data[i]\n        question = example[\"question\"]\n        # using only the first answer.\n        answer = example[\"answers\"][j]\n        q_id = example[\"question_id\"]\n#         if self.make_doc_function is not None:\n#             self.document_cache[q_id] = self.document_cache.get(q_id, self.make_doc_function(example[\"title\"]))\n        document = self.document_cache[q_id] \n        # return from dict document_cache { question_id : ctxs_dense}\n        # Done\n        in_st = \"question: {} context: {}\".format(\n            question.lower().strip(), document.lower().strip(),\n        )\n        out_st = answer\n        return (in_st, out_st)\n\n    def __getitem__(self, idx):\n        return self.make_example(idx)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:38:21.784551Z","iopub.execute_input":"2023-02-12T00:38:21.784933Z","iopub.status.idle":"2023-02-12T00:38:21.796567Z","shell.execute_reply.started":"2023-02-12T00:38:21.784900Z","shell.execute_reply":"2023-02-12T00:38:21.795140Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n    model.eval()\n    # make iterator\n    train_sampler = SequentialSampler(dataset)\n    model_collate_fn = functools.partial(\n        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\"\n    )\n    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    with torch.no_grad():\n        for step, batch_inputs in enumerate(epoch_iterator):\n            pre_loss = model(**batch_inputs)[0]\n            loss = pre_loss.sum() / pre_loss.shape[0]\n            loc_loss += loss.item()\n            loc_steps += 1\n            if step % args.print_freq == 0:\n                print(\n                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n                        step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n                    )\n                )\n    print(\"Total \\t L: {:.3f} \\t -- {:.3f}\".format(loc_loss / loc_steps, time() - st_time,))","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:35:56.795400Z","iopub.execute_input":"2023-02-12T00:35:56.795774Z","iopub.status.idle":"2023-02-12T00:35:56.806206Z","shell.execute_reply.started":"2023-02-12T00:35:56.795742Z","shell.execute_reply":"2023-02-12T00:35:56.804910Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# def query_qa_dense_index(\n#     question, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10, min_length=20, device=\"cuda:0\"\n# ):\n# #     q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n# #     D, I = wiki_index.search(q_rep, 2 * n_results)\n# #     res_passages = [wiki_passages[int(i)] for i in I[0]]\n#     support_doc = \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in res_passages])\n    \n#     return support_doc","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:07:17.484026Z","iopub.execute_input":"2023-02-09T06:07:17.484926Z","iopub.status.idle":"2023-02-09T06:07:17.495505Z","shell.execute_reply.started":"2023-02-09T06:07:17.484829Z","shell.execute_reply":"2023-02-09T06:07:17.494095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def format_doc(dataset):\n#     eli5_valid_docs = []\n#     for example in val:\n#         support_doc = \"<P> \" + \" <P> \".join([str(p) for p in example['ctxs']])\n#         eli5_valid_docs += [(example['question_id'], support_doc)]\n#     return eli5_valid_docs","metadata":{"execution":{"iopub.status.busy":"2023-02-05T19:23:13.716039Z","iopub.execute_input":"2023-02-05T19:23:13.716599Z","iopub.status.idle":"2023-02-05T19:23:13.722017Z","shell.execute_reply.started":"2023-02-05T19:23:13.716562Z","shell.execute_reply":"2023-02-05T19:23:13.720865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eli5_valid_docs[0]","metadata":{"execution":{"iopub.status.busy":"2023-02-02T07:47:07.227495Z","iopub.execute_input":"2023-02-02T07:47:07.227875Z","iopub.status.idle":"2023-02-02T07:47:07.232289Z","shell.execute_reply.started":"2023-02-02T07:47:07.227843Z","shell.execute_reply":"2023-02-02T07:47:07.231257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save for later load","metadata":{}},{"cell_type":"code","source":"# with open(r\"/kaggle/working/eli5_valid_docs.json\", 'w') as f:\n#     f.write(json.dumps(eli5_valid_docs))","metadata":{"execution":{"iopub.status.busy":"2023-02-02T07:24:35.860108Z","iopub.execute_input":"2023-02-02T07:24:35.860611Z","iopub.status.idle":"2023-02-02T07:24:35.940879Z","shell.execute_reply.started":"2023-02-02T07:24:35.860557Z","shell.execute_reply":"2023-02-02T07:24:35.939536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # pre-computing support documents\n# eli5_train_docs = []\n# for example in train:\n#     support_doc = \"<P> \" + \" <P> \".join([str(p) for p in example['ctxs']])\n#     eli5_train_docs += [(example['question_id'], support_doc)]","metadata":{"execution":{"iopub.status.busy":"2023-02-02T07:23:03.965164Z","iopub.execute_input":"2023-02-02T07:23:03.965722Z","iopub.status.idle":"2023-02-02T07:23:10.228132Z","shell.execute_reply.started":"2023-02-02T07:23:03.965674Z","shell.execute_reply":"2023-02-02T07:23:10.227128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(r\"/kaggle/working/eli5_train_docs.json\", 'w') as f:\n#     f.write(json.dumps(eli5_train_docs))","metadata":{"execution":{"iopub.status.busy":"2023-02-02T07:24:59.730603Z","iopub.execute_input":"2023-02-02T07:24:59.731065Z","iopub.status.idle":"2023-02-02T07:25:15.963383Z","shell.execute_reply.started":"2023-02-02T07:24:59.731016Z","shell.execute_reply":"2023-02-02T07:25:15.961936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"s2s_args = ArgumentsS2S()\n\neli5_train_docs = json.load(open('/kaggle/input/dataprepare/eli5_train_docs.json'))\neli5_valid_docs = json.load(open('/kaggle/input/dataprepare/eli5_valid_docs.json'))\n\n                                ## my train and val          # question_id, doc_compute_dense\ns2s_train_dset = ELI5DatasetS2S(train, document_cache=dict([(k, d) for k, d in eli5_train_docs]))\ns2s_valid_dset = ELI5DatasetS2S(val, document_cache=dict([(k, d) for k, d in eli5_valid_docs]), training=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:38:27.316938Z","iopub.execute_input":"2023-02-12T00:38:27.317324Z","iopub.status.idle":"2023-02-12T00:38:40.420004Z","shell.execute_reply.started":"2023-02-12T00:38:27.317291Z","shell.execute_reply":"2023-02-12T00:38:40.418911Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"s2s_valid_dset[0]","metadata":{"execution":{"iopub.status.busy":"2023-02-11T05:41:50.914262Z","iopub.execute_input":"2023-02-11T05:41:50.914712Z","iopub.status.idle":"2023-02-11T05:41:50.924051Z","shell.execute_reply.started":"2023-02-11T05:41:50.914675Z","shell.execute_reply":"2023-02-11T05:41:50.922997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa_s2s_tokenizer, pre_model = make_qa_s2s_model(\n    model_name=\"facebook/bart-base\",\n    from_file=None,\n    device=\"cuda:0\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:38:40.421929Z","iopub.execute_input":"2023-02-12T00:38:40.422410Z","iopub.status.idle":"2023-02-12T00:39:13.462292Z","shell.execute_reply.started":"2023-02-12T00:38:40.422373Z","shell.execute_reply":"2023-02-12T00:39:13.461152Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211806b43e374e0b9991756eb6b1e9c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf5d8724d9c94eacb00a5d70c0d16a10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1bccaa424824c64898767f0925c8e2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f5518619fd42368e21519c1d18b84f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/532M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8618492887ce4c51b3ee934ce00cf1f3"}},"metadata":{}}]},{"cell_type":"code","source":"# import os\n# s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n# s2s_scheduler = get_linear_schedule_with_warmup(\n# s2s_optimizer,\n# num_warmup_steps=400,\n# num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n# )\n# m_save_dict = {\n#             \"model\": qa_s2s_model.module.state_dict()\n#                     if hasattr(qa_s2s_model, \"module\") else qa_s2s_model.state_dict(),\n#             \"optimizer\": s2s_optimizer.state_dict(),\n#             \"scheduler\": s2s_scheduler.state_dict(),\n#         }\n# print(\"Saving model {}\".format(s2s_args.model_save_name))\n# eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n# torch.save(m_save_dict, \"eli5_bart_model_0.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-02-02T19:23:32.060839Z","iopub.execute_input":"2023-02-02T19:23:32.061211Z","iopub.status.idle":"2023-02-02T19:24:56.093256Z","shell.execute_reply.started":"2023-02-02T19:23:32.061177Z","shell.execute_reply":"2023-02-02T19:24:56.091726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa_s2s_model = torch.nn.DataParallel(pre_model)\ntrain_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args)","metadata":{"execution":{"iopub.status.busy":"2023-02-12T00:39:16.712675Z","iopub.execute_input":"2023-02-12T00:39:16.713139Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":" 0     0 of 100549 \t L: 6.134 \t -- 6.639\n 0     1 of 100549 \t L: 6.339 \t -- 7.516\n 0   100 of 100549 \t L: 6.096 \t -- 99.965\n 0   200 of 100549 \t L: 5.534 \t -- 202.945\n 0   300 of 100549 \t L: 5.074 \t -- 305.355\n 0   400 of 100549 \t L: 4.827 \t -- 408.529\n 0   500 of 100549 \t L: 4.632 \t -- 511.759\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# !pip install nlp","metadata":{"execution":{"iopub.status.busy":"2023-02-05T18:44:07.857704Z","iopub.execute_input":"2023-02-05T18:44:07.858148Z","iopub.status.idle":"2023-02-05T18:44:22.996239Z","shell.execute_reply.started":"2023-02-05T18:44:07.858115Z","shell.execute_reply":"2023-02-05T18:44:22.994847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import nlp\n# eli5 = nlp.load_dataset('eli5')","metadata":{"execution":{"iopub.status.busy":"2023-02-06T14:49:01.004098Z","iopub.execute_input":"2023-02-06T14:49:01.004467Z","iopub.status.idle":"2023-02-06T14:49:01.009167Z","shell.execute_reply.started":"2023-02-06T14:49:01.004437Z","shell.execute_reply":"2023-02-06T14:49:01.008103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate answer from input \"question: ... context: <p> ...\"\ndef qa_s2s_generate(\n    question_doc,\n    qa_s2s_model,\n    qa_s2s_tokenizer,\n    num_answers=1,\n    num_beams=None,\n    min_len=64,\n    max_len=256,\n    do_sample=False,\n    temp=1.0,\n    top_p=None,\n    top_k=None,\n    max_input_length=512,\n    device=\"cuda:0\",\n):\n    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, max_input_length, device=device,)\n    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n    \n    generated_ids = qa_s2s_model.generate(\n        input_ids=model_inputs[\"input_ids\"],\n        attention_mask=model_inputs[\"attention_mask\"],\n        min_length=min_len,\n        max_length=max_len,\n        ###\n        do_sample=do_sample,\n        ###\n        early_stopping=True,\n        num_beams=1 if do_sample else n_beams,\n        temperature=temp,\n        ###\n        top_k=top_k,\n        top_p=top_p,\n        ###\n        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n        no_repeat_ngram_size=3,\n        num_return_sequences=num_answers,\n        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n    )\n    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]\n\n# beam 6, top p 50","metadata":{"execution":{"iopub.status.busy":"2023-02-12T21:42:02.506264Z","iopub.execute_input":"2023-02-12T21:42:02.507336Z","iopub.status.idle":"2023-02-12T21:42:02.516623Z","shell.execute_reply.started":"2023-02-12T21:42:02.507294Z","shell.execute_reply":"2023-02-12T21:42:02.515473Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"predicted = []\nreference = []\n\n\n# Generate answers for the full test set\nfor example in val:\n    # create support document with the dense index\n    question = example['question']\n    doc = \"<P> \" + \" <P> \".join([str(p) for p in example['ctxs']])\n    # concatenate question and support document into BART input\n    question_doc = \"question: {} context: {}\".format(question, doc)\n    \n    # generate an answer with beam search\n    answer = qa_s2s_generate(\n                            ####\n            question_doc, pre_model, qa_s2s_tokenizer,\n            num_answers=1,\n            num_beams=8,\n            min_len=96,\n            max_len=256,\n            max_input_length=1024,\n            device=\"cuda:0\"\n    )[0]\n    \n    predicted += [answer]\n\n    reference += [example['answers']]","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:08:56.840631Z","iopub.execute_input":"2023-02-09T06:08:56.841023Z","iopub.status.idle":"2023-02-09T06:50:18.451323Z","shell.execute_reply.started":"2023-02-09T06:08:56.840991Z","shell.execute_reply":"2023-02-09T06:50:18.450272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# abc = []\n# abc += [val[0]['answers']]\n# abc+= [val[1]['answers']]\n# abc","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:57:11.635472Z","iopub.execute_input":"2023-02-09T08:57:11.635914Z","iopub.status.idle":"2023-02-09T08:57:11.646678Z","shell.execute_reply.started":"2023-02-09T08:57:11.635878Z","shell.execute_reply":"2023-02-09T08:57:11.645456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r\"/kaggle/working/pred_ref_t5.json\", 'w') as f:\n    f.write(json.dumps([predicted, reference]))","metadata":{"execution":{"iopub.status.busy":"2023-02-12T19:37:51.876977Z","iopub.execute_input":"2023-02-12T19:37:51.877346Z","iopub.status.idle":"2023-02-12T19:37:52.134982Z","shell.execute_reply.started":"2023-02-12T19:37:51.877314Z","shell.execute_reply":"2023-02-12T19:37:52.132967Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1808745709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"/kaggle/working/pred_ref_t5.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'predicted' is not defined"],"ename":"NameError","evalue":"name 'predicted' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:09:01.400765Z","iopub.execute_input":"2023-02-12T22:09:01.401713Z","iopub.status.idle":"2023-02-12T22:09:16.695018Z","shell.execute_reply.started":"2023-02-12T22:09:01.401611Z","shell.execute_reply":"2023-02-12T22:09:16.693509Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge_score) (0.15.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge_score) (3.8.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge_score) (1.21.6)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge_score) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (2021.11.10)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (1.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->rouge_score) (6.0.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (3.8.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=12f6d46b18cdcbeb308f4e2a1d93cbe998f38b23835145bbb00258f350edf6c4\n  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nlp","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:09:37.374201Z","iopub.execute_input":"2023-02-12T22:09:37.374752Z","iopub.status.idle":"2023-02-12T22:09:48.072659Z","shell.execute_reply.started":"2023-02-12T22:09:37.374721Z","shell.execute_reply":"2023-02-12T22:09:48.071089Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nlp in /opt/conda/lib/python3.7/site-packages (0.4.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from nlp) (2.28.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from nlp) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from nlp) (1.3.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from nlp) (4.64.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from nlp) (3.2.0)\nRequirement already satisfied: pyarrow>=0.16.0 in /opt/conda/lib/python3.7/site-packages (from nlp) (5.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from nlp) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from nlp) (3.7.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2022.12.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->nlp) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->nlp) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport nlp","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:11:34.322806Z","iopub.execute_input":"2023-02-12T22:11:34.323213Z","iopub.status.idle":"2023-02-12T22:11:34.328611Z","shell.execute_reply.started":"2023-02-12T22:11:34.323177Z","shell.execute_reply":"2023-02-12T22:11:34.327419Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/eval-t5-2/pred_ref_t5.json', 'r')  as f:\n    pred_ref_t5 = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:33:22.632287Z","iopub.execute_input":"2023-02-12T22:33:22.633442Z","iopub.status.idle":"2023-02-12T22:33:22.705084Z","shell.execute_reply.started":"2023-02-12T22:33:22.633385Z","shell.execute_reply":"2023-02-12T22:33:22.703904Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"predicted_load = pred_ref_t5[0]\nreference_load = pred_ref_t5[1]\nprint(len(predicted_load), len(reference_load))","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:33:25.023663Z","iopub.execute_input":"2023-02-12T22:33:25.024067Z","iopub.status.idle":"2023-02-12T22:33:25.033293Z","shell.execute_reply.started":"2023-02-12T22:33:25.024034Z","shell.execute_reply":"2023-02-12T22:33:25.032323Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"1507 1507\n","output_type":"stream"}]},{"cell_type":"code","source":"reference_load[4][0]","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:33:26.278394Z","iopub.execute_input":"2023-02-12T22:33:26.279316Z","iopub.status.idle":"2023-02-12T22:33:26.286962Z","shell.execute_reply.started":"2023-02-12T22:33:26.279268Z","shell.execute_reply":"2023-02-12T22:33:26.285787Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'Cannabis is a depressant drug, which means it slows down messages travelling between your brain and body. When large doses of cannabis are taken, it may also produce hallucinogenic effects. Cannabis can cause: reduced coordination, slower reaction times, slower information processing, confusion, changes in vision, hearing, and time and space perception. A person who has been using cannabis may think that they will be able to drive safely. However, the cannabis may have affected their view and experience of reality, and their judgement. Their actions and responses may be quite different to what is actually needed, but they may not be aware of how much their driving skills have been affected. Even after a small amount of cannabis you should not drive for at least 5 hours. (From:  URL_0 )'"},"metadata":{}}]},{"cell_type":"code","source":"reference_load_ans = [reference_load[i][0] for i in range(len(reference_load))]","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:33:26.835324Z","iopub.execute_input":"2023-02-12T22:33:26.835722Z","iopub.status.idle":"2023-02-12T22:33:26.842998Z","shell.execute_reply.started":"2023-02-12T22:33:26.835687Z","shell.execute_reply":"2023-02-12T22:33:26.841813Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"reference_load_ans[1]","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:33:27.258077Z","iopub.execute_input":"2023-02-12T22:33:27.259289Z","iopub.status.idle":"2023-02-12T22:33:27.266810Z","shell.execute_reply.started":"2023-02-12T22:33:27.259239Z","shell.execute_reply":"2023-02-12T22:33:27.265712Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'It is water vapor and ice. They are produced from the hot engine exhaust in the cold atmosphere. Water vapor from the engine exhaust mixed with unburnt particulate in the jet fuel gives the surrounding moist air something to latch onto and ice crystals form. Depending on the hight of the aircraft, they can last seconds to hours. If you have seen a running car on a brisk morning, that is a similar effect. The car is too close to the relatively warmer ground that trails do not last for more than a second.'"},"metadata":{}}]},{"cell_type":"code","source":"import nlp\nnlp_rouge = nlp.load_metric('rouge')","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:11:54.323777Z","iopub.execute_input":"2023-02-12T22:11:54.324211Z","iopub.status.idle":"2023-02-12T22:11:56.322168Z","shell.execute_reply.started":"2023-02-12T22:11:54.324174Z","shell.execute_reply":"2023-02-12T22:11:56.321186Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a89bfc5ba5e46388204a152cf3e46d6"}},"metadata":{}}]},{"cell_type":"code","source":"scores = nlp_rouge.compute(\n    predicted_load, reference_load_ans,\n    rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n    use_agregator=False, use_stemmer=False\n)\n# df = pd.DataFrame({\n#     'rouge1': [scores['rouge1'].mid.precision, scores['rouge1'].mid.recall, scores['rouge1'].mid.fmeasure],\n#     'rouge2': [scores['rouge2'].mid.precision, scores['rouge2'].mid.recall, scores['rouge2'].mid.fmeasure],\n#     'rougeL': [scores['rougeL'].mid.precision, scores['rougeL'].mid.recall, scores['rougeL'].mid.fmeasure],\n# }, index=[ 'P', 'R', 'F'])\n# df.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})\nscores['rougeL'][0:5]","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:33:34.963642Z","iopub.execute_input":"2023-02-12T22:33:34.964008Z","iopub.status.idle":"2023-02-12T22:33:51.217557Z","shell.execute_reply.started":"2023-02-12T22:33:34.963976Z","shell.execute_reply":"2023-02-12T22:33:51.216090Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"[Score(precision=0.27631578947368424, recall=0.0846774193548387, fmeasure=0.12962962962962965),\n Score(precision=0.14864864864864866, recall=0.11702127659574468, fmeasure=0.13095238095238096),\n Score(precision=0.12307692307692308, recall=0.08695652173913043, fmeasure=0.1019108280254777),\n Score(precision=0.13846153846153847, recall=0.11842105263157894, fmeasure=0.12765957446808512),\n Score(precision=0.18840579710144928, recall=0.1, fmeasure=0.13065326633165827)]"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nrougeL = pd.DataFrame(scores['rougeL'])\ntop10 = rougeL.sort_values(by=[\"fmeasure\"],ascending=True)[0:10]","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:38:59.132285Z","iopub.execute_input":"2023-02-12T22:38:59.132741Z","iopub.status.idle":"2023-02-12T22:38:59.142784Z","shell.execute_reply.started":"2023-02-12T22:38:59.132707Z","shell.execute_reply":"2023-02-12T22:38:59.141561Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"idx_top_best_10 = list(top10.index)\nidx_top_best_10","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:38:59.504187Z","iopub.execute_input":"2023-02-12T22:38:59.504553Z","iopub.status.idle":"2023-02-12T22:38:59.511382Z","shell.execute_reply.started":"2023-02-12T22:38:59.504521Z","shell.execute_reply":"2023-02-12T22:38:59.510323Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"[1232, 223, 802, 912, 679, 991, 330, 421, 1479, 1082]"},"metadata":{}}]},{"cell_type":"code","source":"for i in  idx_top_best_10:\n    print((predicted_load[i], reference_load_ans[i]))","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:34:25.698683Z","iopub.execute_input":"2023-02-12T22:34:25.699065Z","iopub.status.idle":"2023-02-12T22:34:25.705255Z","shell.execute_reply.started":"2023-02-12T22:34:25.699031Z","shell.execute_reply":"2023-02-12T22:34:25.704193Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"(\"_URL_0_ The rovers don't have to be directed to the suspected source, but they can't confirm or deny any speculations. They aren't going to be able to confirm that there is water on Mars, but it's not likely that there's a source of water on the surface of Mars. It's also possible that a rover may have carried the bacteria Bacillus safensis to Mars.\", \" >  why can’t one of the rovers be directed to the suspected source and simply confirm or deny speculations? Rovers are extremely slow, you can't just drive them to wherever you want. [Opportunity travelled 45km in all its 12 years on Mars]( URL_0 ) We know (for a long time now) that there is indeed [water (ice) on Mars]( URL_1 ), no need to send a probe there just to confirm it.\")\n('If you were on the moon during a total lunar eclipse, you would see the Moon from an angle that is about 2 degrees different from the angle of sight of an observer who sees the Moon on the western horizon. You would see a lunar eclipse when you look up at the Moon, and the Moon would be visible as a reddish ring. If you had a full moon eclipse, then you would be able to see the Earth.', 'You would see a solar eclipse. The earth is in between the moon and sun, so if you were on the moon you would see the earth with the sun behind it.')\n(\"Ignorance of the law isn't a valid excuse for breaking it. It's a good excuse for committing a crime, but it's not a bad excuse in court. The reason is that if ignorance was an excuse, a person charged with criminal offenses or a subject of a civil lawsuit would merely claim that one was unaware of the laws in question to avoid liability, even if that person didn't know what the law in question is.\", 'If it\\'s a legal excuse for a mild crime, it\\'s a valid legal excuse for a serious one. \"I didn\\'t realize massive theft/murder was illegal in this country.\"')\n('March was the first month of the Roman calendar, and it was the third of ten months to be added to the calendar year. February was the month with 28 days, and the month had 28 days. April was the second month in the year. It was a month for the Romans, which was the shortest month for Romans. May was the fourth month of a calendar year, followed by May and May. May is the third month of April.', 'There was a time when the year began in the month containing the beginning of Spring, namely March. This meant that February was the last month of the year, and it originally had 30 days. July and August were renamed for Caesars of Rome and at the time those months were only 30 days. To honor the greatness of the Caesars, those months were extended to 31 days each and the days were taken from the end of the year, which at the time was February.')\n(\"The ps4 and xb1 are x86 based, so you can't just rip the OS and run it in a vm to emulate them. XB1 is based on a CPU, but it's a bit more complicated than a PC. It's hard to rip and run the OS in real mode, and you can run it as a virtual machine. Then you're able to run a computer in the same way as the xB\", \" URL_0  Making everything the guy above me mean jack shit. The reason why you can't just do it is because it's encrypted on the nand flash, but if you figure those keys out, you can run it as an os on a pc. No need for an APU cause you can modify the IRQ addresses\")\n('In a court case, a person who pleads \"not guilty\" is not given a perjury charge (along with their initial charges) because it\\'s a violation of universal principles of right. It\\'s not a criminal offense, but if you\\'re convicted of a crime, you\\'ll be able to plead guilty to the charge. If you are convicted, you can\\'t be convicted.', ' >  Why is everyone that pleads \"not guilty\" in a court case, but later found guilty, not also given a perjury charge (along with their initial charges)? A plea is not sworn testimony from the defendant, it is just the demand of \"prove it\" to the prosecution. Also it would act to derail the intended operation of the legal process; if you are going to be charged with perjury if you lose then why not lie your ass off at every opportunity? If you win you get off and if you lose you are going to be convicted of perjury for defending yourself anyway. And that is what it really comes down to: You have a *right to a legal defense*. To charge you with perjury just for pleading not guilty would violate your fundamental right to legal defense and would be a violation of human rights. You gotta\\' watch out for those.')\n(\"In sex scenes in movies, they don't actually have a sexual relationship with each other, but it's a lot more complicated than a movie. It's the same thing as a film with a camera, and if you're going to be able to get a shot of it, you'll see a picture of a person who is in love with the other person in the movie. If you've ever seen a scene in the film, you can see it in a\", \"ELI5: In death scenes in movies do they ever just actually kill the actor? Haha, it's a movie. It's called acting. As far a boner goes, imagine *pretending* to have sex with a hot chick, but you're at work in front of your boss, all your co-workers, and even a few friends are watching you and critiquing your hump form, your body position, the director telling you to stop randomly because they have to adjust the sound or lights, etc. It wouldn't be as exciting as you may fantasize.\")\n(\"Humans are the only species of carnivores that need to heat raw meat before it's safe to consume. It's not safe to eat if it is raw or incompletely cooked, and that's the reason why humans aren't able to scavenge and hunt. The only species that can eat raw meat is that they have a very high caloric density, which is why they're the only omnivore.\", 'For the most part we can eat raw meat too. We are the only species that saves are raw meat for days/weeks/months to eat later and that is what makes us have to cook it. We could eat raw meat assuming we eat the meat directly after the animal was killed. It is not the raw meat that is dangerous, it is what grows on the meat after it is dead.')\n('The Hoover dam was built by the US Army Corps of Engineers. The dam was created by building a rock and stone \"crib\" and lining the middle with earthen fill, allowing the dam to be able to store and store water. It was designed to keep the dam from leaking water from the dam. Then the dam was plugged into a tunnel and plugged it into the tunnel. The water was pumped out of the dam and pumped into the water.', 'First, they had to divert away the water. This is usually done by building a tunnel upstream and letting the water flow through it instead of the construction site. Next, they form the dam and let the concrete set for a couple of weeks. After the concrete is sufficiently strong, the tunnel is plugged up and the water is directed back to the dam.')\n(\"_URL_0_ This is why we can't just breed more bees until there are enough to sustain the population. It's a good idea to be able to live in a larger colony, but it's not a bad idea for a bee to have a better chance of surviving a colony if there's enough to survive the colony. So if you're a native bee, you'll have to eat a lot of honey.\", 'Bees don\\'t reproduce very quickly. You can\\'t just \"breed more\" because it would take a while. It would also not solve the underlying problems, so you would have a lot of bees and then you would go to having very few bees very quickly.')\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:13:51.373673Z","iopub.execute_input":"2023-02-12T22:13:51.374157Z","iopub.status.idle":"2023-02-12T22:14:03.183325Z","shell.execute_reply.started":"2023-02-12T22:13:51.374102Z","shell.execute_reply":"2023-02-12T22:14:03.181974Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge) (1.15.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from nltk import PorterStemmer\nfrom rouge import Rouge\nfrom spacy.lang.en import English\nfrom time import time\nfrom spacy.tokenizer import Tokenizer\nimport pandas as pd\n\nstemmer = PorterStemmer()\nrouge = Rouge()\nnlp = English()\n# Create a blank Tokenizer with just the English vocab\ntokenizer = Tokenizer(nlp.vocab)\n# Create a Tokenizer with the default settings for English\n# including punctuation rules and exceptions\n# tokenizer = nlp.Defaults.create_tokenizer(nlp)\n\ndef compute_rouge_eli5(compare_list):\n    preds = [\" \".join([stemmer.stem(str(w))\n                       for w in tokenizer(pred)])\n             for gold, pred in compare_list]\n    golds = [\" \".join([stemmer.stem(str(w))\n                       for w in tokenizer(gold)])\n             for gold, pred in compare_list]\n    scores = rouge.get_scores(preds, golds, avg=True)\n    return scores\n    \n\ncompare_list = [(g, p) for p, g in zip(predicted_load, reference_load_ans)]\nscores = compute_rouge_eli5(compare_list)\ndf = pd.DataFrame({\n    'rouge1': [scores['rouge-1']['p'], scores['rouge-1']['r'], scores['rouge-1']['f']],\n    'rouge2': [scores['rouge-2']['p'], scores['rouge-2']['r'], scores['rouge-2']['f']],\n    'rougeL': [scores['rouge-l']['p'], scores['rouge-l']['r'], scores['rouge-l']['f']],\n}, index=[ 'P', 'R', 'F'])\ndf.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})","metadata":{"execution":{"iopub.status.busy":"2023-02-12T22:16:24.056186Z","iopub.execute_input":"2023-02-12T22:16:24.057160Z","iopub.status.idle":"2023-02-12T22:16:49.213172Z","shell.execute_reply.started":"2023-02-12T22:16:24.057096Z","shell.execute_reply":"2023-02-12T22:16:49.208800Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7f61d0d56b50>","text/html":"<style type=\"text/css\">\n</style>\n<table id=\"T_bc9d0_\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th class=\"col_heading level0 col0\" >rouge1</th>\n      <th class=\"col_heading level0 col1\" >rouge2</th>\n      <th class=\"col_heading level0 col2\" >rougeL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_bc9d0_level0_row0\" class=\"row_heading level0 row0\" >P</th>\n      <td id=\"T_bc9d0_row0_col0\" class=\"data row0 col0\" >0.3150</td>\n      <td id=\"T_bc9d0_row0_col1\" class=\"data row0 col1\" >0.0535</td>\n      <td id=\"T_bc9d0_row0_col2\" class=\"data row0 col2\" >0.2831</td>\n    </tr>\n    <tr>\n      <th id=\"T_bc9d0_level0_row1\" class=\"row_heading level0 row1\" >R</th>\n      <td id=\"T_bc9d0_row1_col0\" class=\"data row1 col0\" >0.2172</td>\n      <td id=\"T_bc9d0_row1_col1\" class=\"data row1 col1\" >0.0380</td>\n      <td id=\"T_bc9d0_row1_col2\" class=\"data row1 col2\" >0.1935</td>\n    </tr>\n    <tr>\n      <th id=\"T_bc9d0_level0_row2\" class=\"row_heading level0 row2\" >F</th>\n      <td id=\"T_bc9d0_row2_col0\" class=\"data row2 col0\" >0.2288</td>\n      <td id=\"T_bc9d0_row2_col1\" class=\"data row2 col1\" >0.0376</td>\n      <td id=\"T_bc9d0_row2_col2\" class=\"data row2 col2\" >0.2043</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"# idx = 100\n# print(\"Question:   {}\".format(val[idx]['question']))\n# print(\"Prediction: {}\".format(predicted_load[idx]))\n# print(\"Reference:  {}\".format(reference_load[idx]))","metadata":{"execution":{"iopub.status.busy":"2023-02-09T08:11:30.073240Z","iopub.execute_input":"2023-02-09T08:11:30.073940Z","iopub.status.idle":"2023-02-09T08:11:30.082116Z","shell.execute_reply.started":"2023-02-09T08:11:30.073884Z","shell.execute_reply":"2023-02-09T08:11:30.081098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-02-09T07:50:58.795552Z","iopub.execute_input":"2023-02-09T07:50:58.796039Z","iopub.status.idle":"2023-02-09T07:50:58.806406Z","shell.execute_reply.started":"2023-02-09T07:50:58.796000Z","shell.execute_reply":"2023-02-09T07:50:58.804361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-02-09T07:51:02.021460Z","iopub.execute_input":"2023-02-09T07:51:02.021890Z","iopub.status.idle":"2023-02-09T07:51:02.032538Z","shell.execute_reply.started":"2023-02-09T07:51:02.021855Z","shell.execute_reply":"2023-02-09T07:51:02.030820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import random","metadata":{"execution":{"iopub.status.busy":"2023-02-06T14:48:30.416100Z","iopub.execute_input":"2023-02-06T14:48:30.416493Z","iopub.status.idle":"2023-02-06T14:48:30.422855Z","shell.execute_reply.started":"2023-02-06T14:48:30.416462Z","shell.execute_reply":"2023-02-06T14:48:30.420988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install psycopg2-binary \n!pip -q install transformers\n!pip -q install python-dotenv","metadata":{"execution":{"iopub.status.busy":"2023-02-12T21:39:56.865091Z","iopub.execute_input":"2023-02-12T21:39:56.865467Z","iopub.status.idle":"2023-02-12T21:40:27.751569Z","shell.execute_reply.started":"2023-02-12T21:39:56.865433Z","shell.execute_reply":"2023-02-12T21:40:27.750335Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport dotenv\nimport psycopg2\nimport time\nfrom transformers import  AutoModel, AutoTokenizer\n\n# from src.utils import *\n# from src.train_utils import *\n\ndotenv.load_dotenv()\n\nDBNAME=os.getenv(\"DBNAME\", \"wiki_db\")\nHOST=os.getenv(\"HOST\", \"124.158.12.207\")\nPORT=os.getenv(\"PORT\", \"15433\")\nUSER=os.getenv(\"USER\", \"gradlab\")\nPWD=os.getenv(\"PASSWORD\", \"baldarg\")\n# TB_CLIENT=os.getenv(\"TB_CLIENT\",\"client_tb\")\nTB_WIKI=os.getenv(\"TB_WIKI\", \"wiki_tb\")\n# MSD_WIKI = bool(os.getenv(\"MSD_WIKI\", False))\n\n#TODO: Use this function\n\ndef get_embds_qs(question, tokenizer, model, device):\n    # Tokenize sentences\n    q_toks = tokenizer.batch_encode_plus(question, max_length=128, pad_to_max_length=True)\n    q_ids, q_mask = (\n        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n    )\n    with torch.no_grad():\n        q_reps = model.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n\n    return q_reps\n\ndef query_embd(embd, limit_doc=3, ):\n    embd = str(list(embd.cpu().detach().numpy().reshape(-1)))\n    try:\n        connection = psycopg2.connect(dbname=DBNAME,host=HOST,port=PORT,user=USER,password=PWD)\n        cursor = connection.cursor()\n        aemb_sql = f\"\"\"\n                        SET LOCAL ivfflat.probes = 3;\n                        SELECT content \n                        FROM {TB_WIKI}\n                        ORDER BY embedd <#> %s LIMIT %s;\n                    \"\"\"\n        cursor.execute(aemb_sql,(embd, limit_doc))\n        connection.commit()\n        rows = cursor.fetchall()\n\n        if connection: \n            cursor.close()\n            connection.close()\n        \n        return rows\n        \n    except (Exception, psycopg2.Error) as error: \n        print(\"Failed query record from database {}\".format(error))\n\n# def load_model_qs(\n#     pretrain_name= r\"vblagoje/dpr-question_encoder-single-lfqa-wiki\", \n#     device = torch.device(\"cuda:0\")\n#     ):\n#     qs_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(pretrain_name)\n#     qs_model = DPRQuestionEncoder.from_pretrained(pretrain_name)\n#     qs_model.to(device)\n    \n#     return qs_model, qs_tokenizer\n\n\ndef retrieve(question, qs_embedder, qs_tokenizer, device, limit_doc=10):\n    # question_embd = get_ embds_qs(qs_embedder, qs_tokenizer, question, device=device)\n    question_embd = get_embds_qs([question], qs_tokenizer, qs_embedder, device=device)\n    documents_wiki = query_embd(question_embd, limit_doc=limit_doc)\n    return [doc[-1] for doc in documents_wiki]\n\nif __name__ == \"__main__\":\n\n    qar_tokenizer = AutoTokenizer.from_pretrained('yjernite/retribert-base-uncased')\n    qar_model = AutoModel.from_pretrained('yjernite/retribert-base-uncased').to('cuda:0')\n    _ = qar_model.eval()\n\n    qa_s2s_tokenizer, pre_model = make_qa_s2s_model(model_name=\"t5-small\",\n                                                from_file= r\"/kaggle/input/model-t5/t5-small_model.pth\",\n                                                device=\"cuda:0\")\n\n    while(True):\n        question = input(\"\\nUSER:\")\n        if question == \"[EXIT]\":\n            break\n        else:   \n            doc_10 = retrieve(question, qar_model, qar_tokenizer, \"cuda:0\", limit_doc=10)\n            doc = \"<P> \" + \" <P> \".join([p for p in doc_10])\n            question_doc = \"question: {} context: {}\".format(question, doc)\n\n            # generate an answer with beam search\n            # start = time.time()\n            answer = qa_s2s_generate(\n                    question_doc, pre_model, qa_s2s_tokenizer,\n                    num_answers=1,\n                    num_beams=6,\n                    min_len=3,\n                    max_len=100,\n                    max_input_length=1024,\n                    device=\"cuda:0\")[0]       \n            # end = time.time()\n\n            print(\"\\nBOT:\", answer.replace(\"\\n\", \"\"))\n            # print(\"time to generate answer: {}\".format(end-start))\n","metadata":{"execution":{"iopub.status.busy":"2023-02-12T21:55:03.804320Z","iopub.execute_input":"2023-02-12T21:55:03.804886Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of RetriBertModel were not initialized from the model checkpoint at yjernite/retribert-base-uncased and are newly initialized: ['bert_query.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d99d8a8c2f45a0befb75bf77f25fbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84f036163e24c85a400917d04763afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9341977684b54995a1552e717f78af0c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eced1f3b2b1a4361bbcf28efca0b977c"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"\nUSER: If you could vacation anywhere in the world, where would it be?\n"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\nBOT: _URL_0_\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUSER: What is your go-to board game?\n"},{"name":"stdout","text":"\nBOT: _URL_0_\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUSER: Which country is the largest cheese producer in the world?\n"},{"name":"stdout","text":"\nBOT: China is the largest cheese producer in the world. It's the largest producer of cheese, and it's not a big deal.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUSER: What is the largest country in Africa?\n"},{"name":"stdout","text":"\nBOT: Britain is the largest country in Africa. It's a big country in the world, and it's the largest nation in Africa, and the largest in Africa is the world's largest country.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUSER: What is the currency in South Africa?\n"},{"name":"stdout","text":"\nBOT: China is the currency in South Africa. It's not a currency, but it's the currency of South Africa, which is a foreign currency.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}