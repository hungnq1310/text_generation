{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\Work\\Baseline_V1\\data_task_1_train\\data_task_1_train.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': '3e9esy',\n",
       " 'question': \"With home backup batteries gaining some momentum through products like the Tesla Powerwall, why aren't flywheel batteries being discussed as an alternative?\",\n",
       " 'answers': [\"Flywheels big enough to store power for a house are big heavy things that spin very, very fast and contain a huge amount of energy. Like enough energy to launch big pieces of metal clear into the next county if they get unbalanced.\\n\\nThat's why you don't see them suggested as an alternative for home use.\"],\n",
       " 'ctxs': ['PV systems, due to their high reliability, low self discharge and investment and maintenance costs, despite shorter lifetime and lower energy density. Lithium-ion batteries have the potential to replace lead-acid batteries in the near future, as they are being intensively developed and lower prices are expected due to economies of scale provided by large production facilities such as the Gigafactory 1. In addition, the Li-ion batteries of plug-in electric cars may serve as a future storage devices in a vehicle-to-grid system. Since most vehicles are parked an average of 95% of the time, their batteries could be used to let',\n",
       "  \"Fluence Z.E. cars had been deployed in Israel and only around 400 units in Denmark. Under Better Place's business model, the company owned the batteries, so the court liquidator had to decide what to do with customers who did not have ownership of the battery and risked being left with a useless car. Tesla Tesla designed its Model S to allow fast battery swapping. In June 2013, Tesla announced its goal of deploying a battery swapping station in each of its supercharging stations. At a demonstration event, Tesla showed that a battery swap operation with the Model S took just\",\n",
       "  \"The power electronics such as the motor, motor controller, AC motor controller, and high voltage DCDC converter are liquid-cooled. The batteries and battery charger are air-cooled. Early life problems There were numerous problems with the NiMH Ranger associated with an inability to accept a charge in hot environmental conditions, and some other problems requiring replacement of major components, but Ford successfully addressed these problems early in the vehicle's life cycle. There were some range issues around the 25,000-mile (40,200\\xa0km) service life with the batteries, and due to the great expense of these batteries, Ford elected not to fix this range\",\n",
       "  'throughout the USA.\\nIn March 2009, Tesla Motors announced a partnership to deploy battery swap stations among their existing Supercharger network  to service their Model S platform cars. Tesla abandoned battery swapping citing low demand.\\nThe Nation-E\\'s Angel Car system is a portable unit, containing a lithium-ion battery, that stores energy and is used as an emergency charger for electric cars that run out of power. It designed to be a solution to \"range anxiety\" without the deployment of significant new infrastructure, and provides fast-charging services for all known Evs equipped with a fast-charging socket, including hybrid cars.',\n",
       "  'operated in Chicago for owners of Milburn Light Electric cars who also could buy the vehicle without the batteries.\\nElectric forklifts have used battery swapping since at least 1946 and a rapid battery replacement system was implemented to help maintain 50 electric buses at the 2008 Summer Olympics in China. Better Place business model Better Place implemented a business model wherein customers entered into subscriptions to purchase driving distance similar to the mobile telephone industry from which customers contract for minutes of airtime.  The initial cost of an electric vehicle might also have been subsidized by the ongoing per-distance revenue',\n",
       "  'to efficiency. For instance, they are used in stand-alone (\"off-grid\") photovoltaic (PV) systems to prevent batteries from discharging through the solar panels at night, called \"blocking diodes\". They are also used in grid-connected systems with multiple strings connected in parallel, in order to prevent reverse current flowing from adjacent strings through shaded strings if the \"bypass diodes\" have failed. Switched-mode power supplies Schottky diodes are also used as rectifiers in switched-mode power supplies. The low forward voltage and fast recovery time leads to increased efficiency.\\nThey can also be used in power supply \"OR\"ing circuits in products that have both an',\n",
       "  'keep electric vehicles in demand, Better Place was going to try to keep the vehicles competitive with the other cars on the market. By building infrastructure that made owning an electric car more practical, they hoped to increase demand.\\nThe first prototype battery switch station opened in Yokohama, Japan on May 14, 2009, was designed by Yoav Heichal, chief engineer for Better Place research and development group.\\nThe company signed an agreement with Dor Alon Energy to install battery replacement points, which would run alongside the petroleum refueling station\\' normal business. Dor Alon CEO, Israel Yaniv, said, \"Dor Alon is the first',\n",
       "  'The Tesla Roadster (2008) and other cars produced by the company used a modified form of traditional lithium-ion \"laptop battery\" cells that can be replaced individually as needed.\\nRecent EVs are utilizing new variations on lithium-ion chemistry that sacrifice specific energy and specific power to provide fire resistance, environmental friendliness,  rapid charging (as quickly as a few minutes), and longer lifespans. These variants (phosphates, titanates, spinels, etc.) have been shown to have a much longer lifetime, with A123 types using  lithium iron phosphate  lasting at least 10+ years and 7000+ charge/discharge cycles, and LG Chem expecting their lithium-manganese',\n",
       "  'system using only one of the two batteries.\\nVintage cars may have 6-volt electrical systems, or may connect the positive terminal of the battery to the chassis. The methods intended for boosting 12-volt, negative-ground vehicles cannot be used in such cases.\\nHybrid vehicles may have a very small 12 volt battery system unsuitable for sourcing the large amount of current required to boost a conventional vehicle.  However, as the 12-volt system of a hybrid vehicle is only required to start up the control system of the vehicle, a very small portable battery may successfully boost a hybrid that has accidentally discharged',\n",
       "  'loads. Although this strategy does not include a purely all-electric mode, early NREL (National Renewable Energy Laboratory) simulations indicate that similar fuel savings as compared to conventional plug-in hybrid battery discharge and charge strategies.  One advantage of a blended mode is that it may afford the vehicle designer the opportunity to use a smaller and less costly battery pack and traction motor.']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length=128, device=\"cuda:0\"):\n",
    "    a_toks = tokenizer.batch_encode_plus(passages, max_length=max_length, pad_to_max_length=True)\n",
    "    a_ids, a_mask = (\n",
    "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        a_reps = qa_embedder.embed_answers(a_ids, a_mask).cpu().type(torch.float)\n",
    "    return a_reps.numpy()\n",
    "\n",
    "def embed_questions_for_retrieval(q_ls, tokenizer, qa_embedder, device=\"cuda:0\"):\n",
    "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=128, pad_to_max_length=True)\n",
    "    q_ids, q_mask = (\n",
    "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
    "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        q_reps = qa_embedder.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n",
    "    return q_reps.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [sample['question'] for sample in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RetriBertModel were not initialized from the model checkpoint at yjernite/retribert-base-uncased and are newly initialized: ['bert_query.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "qar_model = AutoModel.from_pretrained('yjernite/retribert-base-uncased').to('cuda:0')\n",
    "_ = qar_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalQAEmbedder(torch.nn.Module):\n",
    "    def __init__(self, sent_encoder, dim):\n",
    "        super(RetrievalQAEmbedder, self).__init__()\n",
    "        self.sent_encoder = sent_encoder\n",
    "        self.output_dim = 128\n",
    "        self.project_q = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
    "        self.project_a = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    def embed_sentences_checkpointed(self, input_ids, attention_mask, checkpoint_batch_size=-1):\n",
    "        # reproduces BERT forward pass with checkpointing\n",
    "        if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:\n",
    "            return self.sent_encoder(input_ids, attention_mask=attention_mask)[1]\n",
    "        else:\n",
    "            # prepare implicit variables\n",
    "            device = input_ids.device\n",
    "            input_shape = input_ids.size()\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "            head_mask = [None] * self.sent_encoder.config.num_hidden_layers\n",
    "            extended_attention_mask: torch.Tensor = self.sent_encoder.get_extended_attention_mask(\n",
    "                attention_mask, input_shape, device\n",
    "            )\n",
    "\n",
    "            # define function for checkpointing\n",
    "            def partial_encode(*inputs):\n",
    "                encoder_outputs = self.sent_encoder.encoder(inputs[0], attention_mask=inputs[1], head_mask=head_mask,)\n",
    "                sequence_output = encoder_outputs[0]\n",
    "                pooled_output = self.sent_encoder.pooler(sequence_output)\n",
    "                return pooled_output\n",
    "\n",
    "            # run embedding layer on everything at once\n",
    "            embedding_output = self.sent_encoder.embeddings(\n",
    "                input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None\n",
    "            )\n",
    "            # run encoding and pooling on one mini-batch at a time\n",
    "            pooled_output_list = []\n",
    "            for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):\n",
    "                b_embedding_output = embedding_output[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
    "                b_attention_mask = extended_attention_mask[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
    "                pooled_output = checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)\n",
    "                pooled_output_list.append(pooled_output)\n",
    "            return torch.cat(pooled_output_list, dim=0)\n",
    "\n",
    "    def embed_questions(self, q_ids, q_mask, checkpoint_batch_size=-1):\n",
    "        q_reps = self.embed_sentences_checkpointed(q_ids, q_mask, checkpoint_batch_size)\n",
    "        return self.project_q(q_reps)\n",
    "\n",
    "    def embed_answers(self, a_ids, a_mask, checkpoint_batch_size=-1):\n",
    "        a_reps = self.embed_sentences_checkpointed(a_ids, a_mask, checkpoint_batch_size)\n",
    "        return self.project_a(a_reps)\n",
    "\n",
    "    def forward(self, q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=-1):\n",
    "        device = q_ids.device\n",
    "        q_reps = self.embed_questions(q_ids, q_mask, checkpoint_batch_size)\n",
    "        a_reps = self.embed_answers(a_ids, a_mask, checkpoint_batch_size)\n",
    "        compare_scores = torch.mm(q_reps, a_reps.t())\n",
    "        loss_qa = self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))\n",
    "        loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n",
    "        loss = (loss_qa + loss_aq) / 2\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qa_retriever_model(model_name=\"google/bert_uncased_L-8_H-512_A-8\", from_file=None, device=\"cuda:0\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    # run bert_model on a dummy batch to get output dimension\n",
    "    d_ids = torch.LongTensor(\n",
    "        [[bert_model.config.bos_token_id if bert_model.config.bos_token_id is not None else 1]]\n",
    "    ).to(device)\n",
    "    d_mask = torch.LongTensor([[1]]).to(device)\n",
    "    sent_dim = bert_model(d_ids, attention_mask=d_mask)[1].shape[-1]\n",
    "    qa_embedder = RetrievalQAEmbedder(bert_model, sent_dim).to(device)\n",
    "    if from_file is not None:\n",
    "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
    "        qa_embedder.load_state_dict(param_dict[\"model\"])\n",
    "    return tokenizer, qa_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_qa_dense_index(\n",
    "    question, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10, min_length=20, device=\"cuda:0\"\n",
    "):\n",
    "    q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n",
    "    D, I = wiki_index.search(q_rep, 2 * n_results)\n",
    "    res_passages = [wiki_passages[int(i)] for i in I[0]]\n",
    "    support_doc = \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in res_passages])\n",
    "    res_list = [dict([(k, p[k]) for k in wiki_passages.column_names]) for p in res_passages]\n",
    "    res_list = [res for res in res_list if len(res[\"passage_text\"].split()) > min_length][:n_results]\n",
    "    for r, sc in zip(res_list, D[0]):\n",
    "        r[\"score\"] = float(sc)\n",
    "    return support_doc, res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_questions_for_retrieval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q_rep \u001b[39m=\u001b[39m embed_questions_for_retrieval(questions, tokenizer, qa_embedder)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embed_questions_for_retrieval' is not defined"
     ]
    }
   ],
   "source": [
    "# q_rep = embed_questions_for_retrieval(questions, tokenizer, qa_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Work\\\\Baseline_V1\\\\data_task_1_train\\\\ELI5_ori_processed\\\\ELI5_val_10_doc.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer,  get_linear_schedule_with_warmup\n\u001b[0;32m      8\u001b[0m \u001b[39m# with open(r'D:\\Work\\Baseline_V1\\data_task_1_train\\ELI5_ori_processed\\ELI5_train_10_doc.json') as f:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m#     train = json.load(f)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mWork\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mBaseline_V1\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata_task_1_train\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mELI5_ori_processed\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mELI5_val_10_doc.json\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m     val \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m     14\u001b[0m \u001b[39m#--------------------------------------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Work\\\\Baseline_V1\\\\data_task_1_train\\\\ELI5_ori_processed\\\\ELI5_val_10_doc.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nlp\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer,  get_linear_schedule_with_warmup\n",
    "\n",
    "# with open(r'D:\\Work\\Baseline_V1\\data_task_1_train\\ELI5_ori_processed\\ELI5_train_10_doc.json') as f:\n",
    "#     train = json.load(f)\n",
    "    \n",
    "with open(r'D:\\Work\\Baseline_V1\\data_task_1_train\\ELI5_ori_processed\\ELI5_val_10_doc.json') as f:\n",
    "    val = json.load(f)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "with open(r'D:\\Work\\Baseline_V1\\pred_ref_t5.json', 'r')  as f:\n",
    "    pred_ref_t5 = json.load(f)\n",
    "\n",
    "with open(r'D:\\Work\\Baseline_V1\\prediction_t5_2.json', 'r')  as f:\n",
    "    predict_t5 = json.load(f)\n",
    "\n",
    "predicted_load = pred_ref_t5[0]\n",
    "reference_load = pred_ref_t5[1]\n",
    "reference_load_5samp = reference_load[0:5]\n",
    "\n",
    "\n",
    "for i, sample in enumerate(reference_load_5samp):\n",
    "    reference_load_5samp[i] = sample[0]\n",
    "\n",
    "print(\"------------\")\n",
    "print(\"Prediction's len: {}, Reference's len: {}\".format(len(predict_t5), len(reference_load_5samp)))\n",
    "print(\"------------\")\n",
    "\n",
    "nlp_rouge = nlp.load_metric('rouge')\n",
    "scores = nlp_rouge.compute(\n",
    "    predict_t5, reference_load_5samp,\n",
    "    rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
    "    use_agregator=True, use_stemmer=False\n",
    ")\n",
    "df = pd.DataFrame({\n",
    "    'rouge1': [scores['rouge1'].mid.precision, scores['rouge1'].mid.recall, scores['rouge1'].mid.fmeasure],\n",
    "    'rouge2': [scores['rouge2'].mid.precision, scores['rouge2'].mid.recall, scores['rouge2'].mid.fmeasure],\n",
    "    'rougeL': [scores['rougeL'].mid.precision, scores['rougeL'].mid.recall, scores['rougeL'].mid.fmeasure],\n",
    "}, index=[ 'P', 'R', 'F'])\n",
    "df.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})\n",
    "display(df)\n",
    "\n",
    "# idx = 2\n",
    "# print(\"Question:   {}\".format(val[2]['question']))\n",
    "# print(\"Prediction: {}\".format(predicted_load[idx]))\n",
    "# print(\"Reference:  {}\".format(reference_load[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(predicted_load[idx], reference_load_5samp[idx]))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "print(\"{} \\n {}\".format(predicted_load[idx], reference_load_5samp[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import psycopg2\n",
    "import torch\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer,AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "DBNAME=os.getenv(\"DBNAME\", \"wiki_db\")\n",
    "HOST=os.getenv(\"HOST\", \"124.158.12.207\")\n",
    "PORT=os.getenv(\"PORT\", \"15433\")\n",
    "USER=os.getenv(\"USER\", \"gradlab\")\n",
    "PWD=os.getenv(\"PASSWORD\", \"baldarg\")\n",
    "# TB_CLIENT=os.getenv(\"TB_CLIENT\",\"client_tb\")\n",
    "TB_WIKI=os.getenv(\"TB_WIKI\", \"wiki_tb\")\n",
    "# MSD_WIKI = bool(os.getenv(\"MSD_WIKI\", False))\n",
    "\n",
    "#TODO: Use this function\n",
    "\n",
    "def query_embd(embd, limit_doc=3, ):\n",
    "    embd = str(list(embd.cpu().detach().numpy().reshape(-1)))\n",
    "    try:\n",
    "        connection = psycopg2.connect(dbname=DBNAME,host=HOST,port=PORT,user=USER,password=PWD)\n",
    "        cursor = connection.cursor()\n",
    "        aemb_sql = f\"\"\"\n",
    "                        SET LOCAL ivfflat.probes = 3;\n",
    "                        SELECT content \n",
    "                        FROM {TB_WIKI}\n",
    "                        ORDER BY embedd <#> %s LIMIT %s;\n",
    "                    \"\"\"\n",
    "        cursor.execute(aemb_sql,(embd, limit_doc))\n",
    "        connection.commit()\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        if connection: \n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "        \n",
    "        return rows\n",
    "        \n",
    "    except (Exception, psycopg2.Error) as error: \n",
    "        print(\"Failed query record from database {}\".format(error))\n",
    "\n",
    "def load_model_qs(\n",
    "    pretrain_name=\"vblagoje/dpr-question_encoder-single-lfqa-wiki\", \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    ):\n",
    "    qs_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(pretrain_name)\n",
    "    qs_model = DPRQuestionEncoder.from_pretrained(pretrain_name)\n",
    "    qs_model.to(device)\n",
    "    \n",
    "    return qs_model, qs_tokenizer\n",
    "\n",
    "def get_embds_qs(model, tokenizer, text, device):\n",
    "    # Tokenize sentences\n",
    "    model.eval()\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    return model_output['pooler_output']\n",
    "\n",
    "def make_qa_s2s_model(model_name=\"facebook/bart-base\", from_file=None, device=\"cuda:0\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "    if from_file is not None:\n",
    "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
    "        model.load_state_dict(param_dict[\"model\"])\n",
    "    return tokenizer, model\n",
    "\n",
    "def qa_s2s_generate(question_doc, qa_s2s_model, qa_s2s_tokenizer, num_answers=1, num_beams=None,\n",
    "                    min_len=64, max_len=256, do_sample=False,temp=1.0, top_p=None, top_k=None,\n",
    "                    max_input_length=512, device=\"cuda:0\"):\n",
    "    \n",
    "    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, \n",
    "                                       max_input_length, device=device)\n",
    "    \n",
    "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
    "    model = qa_s2s_model.module if hasattr(qa_s2s_model, 'module') else qa_s2s_model \n",
    "    generated_ids = model.generate( input_ids=model_inputs[\"input_ids\"],\n",
    "                                           attention_mask=model_inputs[\"attention_mask\"],\n",
    "                                           min_length=min_len,max_length=max_len,\n",
    "                                           do_sample=do_sample, early_stopping=True,\n",
    "                                           num_beams=1 if do_sample else n_beams,\n",
    "                                           temperature=temp,top_k=top_k,top_p=top_p,\n",
    "                                           eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
    "                                           no_repeat_ngram_size=3,\n",
    "                                           num_return_sequences=num_answers,\n",
    "                                           decoder_start_token_id=qa_s2s_tokenizer.bos_token_id)\n",
    "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]\n",
    "\n",
    "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=360, device=\"cuda:0\"):\n",
    "    q_ls = [q for q, a in qa_list]\n",
    "    a_ls = [a for q, a in qa_list]\n",
    "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
    "    q_ids, q_mask = (torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
    "                     torch.LongTensor(q_toks[\"attention_mask\"]).to(device))\n",
    "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True)\n",
    "    a_ids, a_mask = (torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
    "                     torch.LongTensor(a_toks[\"attention_mask\"]).to(device))\n",
    "    labels = a_ids[:, 1:].contiguous().clone()\n",
    "    labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
    "    model_inputs = {\"input_ids\": q_ids,\n",
    "                    \"attention_mask\": q_mask,\n",
    "                    \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
    "                    \"labels\": labels}\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_s2s_tokenizer, pre_model = make_qa_s2s_model(model_name=\"facebook/bart-base\",\n",
    "                                                from_file=r\"D:\\Work\\Baseline_V1\\src\\Model\\bart-base_model.pth\",\n",
    "                                                device=\"cpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(question, qs_model, qs_tokenizer, device, limit_doc=10):\n",
    "    question_embd = get_embds_qs(qs_model, qs_tokenizer, question, device=device)\n",
    "    documents_wiki = query_embd(question_embd, limit_doc=limit_doc)\n",
    "    return [doc[-1] for doc in documents_wiki]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(1):\n",
    "    question = input(\"\\nUSER:\")\n",
    "    if question == \"[EXIT]\":\n",
    "        break\n",
    "    else:\n",
    "        doc_5 = retrieve(question, pre_model, qa_s2s_tokenizer, \"cuda:0\", limit_doc=5)\n",
    "        doc = \"<P> \" + \" <P> \".join([p for p in doc_5])\n",
    "        question_doc = \"question: {} context: {}\".format(question, doc)\n",
    "\n",
    "        # generate an answer with beam search\n",
    "        answer = qa_s2s_generate(\n",
    "                question_doc, pre_model, qa_s2s_tokenizer,\n",
    "                num_answers=1,\n",
    "                num_beams=6,\n",
    "                min_len=3,\n",
    "                max_len=100,\n",
    "                max_input_length=1024,\n",
    "                device=\"cuda:0\")[0]\n",
    "        \n",
    "        print(\"\\nBOT:\", answer.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e510f8154290173a8650093344b2e0b86c35bf30a7a4be8ba88df254b45ccb05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
